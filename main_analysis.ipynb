{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6334ff85-2e81-471d-8d4a-f2f478b2227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment Ready for Remi!\n",
      "Session started at: 2026-02-09 22:27:28\n"
     ]
    }
   ],
   "source": [
    "# 1. Install necessary libraries\n",
    "# We use 'quiet' to keep the output clean, remove it if you want to see installation details\n",
    "!pip install pandas nltk textblob --quiet\n",
    "\n",
    "# 2. Import modules\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from textblob import TextBlob # Good for simple sentiment detection (keeping mood positive)\n",
    "\n",
    "# 3. Download required NLTK data for tokenizing text\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True) # Useful corpus for reference\n",
    "\n",
    "# 4. Define a simple configuration for the bot\n",
    "BOT_CONFIG = {\n",
    "    \"bot_name\": \"Remi\", # Short for Reminiscence\n",
    "    \"user_name\": \"User\", # We can change this dynamically later\n",
    "    \"session_start\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "print(f\"✅ Environment Ready for {BOT_CONFIG['bot_name']}!\")\n",
    "print(f\"Session started at: {BOT_CONFIG['session_start']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794f8c8-f357-4fb4-9c5a-dd0f34d16f65",
   "metadata": {},
   "source": [
    "From the code above, \n",
    "\n",
    "pandas: We will use this to save conversation logs (very important for analysis later).\n",
    "\n",
    "textblob: A simple library to detect sentiment. If a user gets agitated (negative sentiment), we can program the bot to change the subject.\n",
    "\n",
    "BOT_CONFIG: A dictionary to store basic settings we can reference later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30713fb0-0faf-4f82-8b96-7598348c1caa",
   "metadata": {},
   "source": [
    "We now want to move to Step 2: Defining the \"Reminiscence Knowledge Base\" This is the brain the bot will draw from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee12d3e6-dbcd-4f8c-bb77-50d7c2688ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Knowledge Base Loaded with topics: ['childhood', 'career', 'hobbies', 'family']\n"
     ]
    }
   ],
   "source": [
    "# Define the topics and specific reminiscence prompts\n",
    "reminiscence_topics = {\n",
    "    \"childhood\": [\n",
    "        \"What was your favorite game to play outside when you were young?\",\n",
    "        \"Do you remember the house you grew up in? What was your favorite room?\",\n",
    "        \"What was your favorite home-cooked meal as a child?\",\n",
    "        \"Did you have any pets growing up? What were their names?\"\n",
    "    ],\n",
    "    \"career\": [\n",
    "        \"What was your very first job? Did you enjoy it?\",\n",
    "        \"What is a professional accomplishment you are most proud of?\",\n",
    "        \"Who was the best boss or mentor you ever had?\",\n",
    "        \"How did your work change over the years?\"\n",
    "    ],\n",
    "    \"hobbies\": [\n",
    "        \"What hobbies have you enjoyed throughout your life?\",\n",
    "        \"Is there a particular song or type of music that always makes you smile?\",\n",
    "        \"Did you enjoy traveling? Where was your favorite place to visit?\",\n",
    "        \"What is a book or movie that had a big impact on you?\"\n",
    "    ],\n",
    "    \"family\": [\n",
    "        \"How did you meet your spouse or partner?\",\n",
    "        \"What is a favorite holiday tradition your family had?\",\n",
    "        \"Tell me about a memorable family vacation.\",\n",
    "        \"What values were most important to your parents?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Simple function to get a random question from a topic\n",
    "def get_prompt(topic):\n",
    "    if topic in reminiscence_topics:\n",
    "        return random.choice(reminiscence_topics[topic])\n",
    "    else:\n",
    "        return \"I'd love to hear more about that. Can you tell me more?\"\n",
    "\n",
    "print(\"✅ Knowledge Base Loaded with topics:\", list(reminiscence_topics.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b020f09-8337-4768-b28a-9d5c1323f0a6",
   "metadata": {},
   "source": [
    "We are keeping the conversation focused on therapeutic areas and later we can add other information like music, school, e.t.c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df44e59-3c60-4071-a3eb-a8207d93938d",
   "metadata": {},
   "source": [
    "Defining The Logic Functions:\n",
    "\n",
    "Next, we are going to build the \"Brain\" It is a function that takes what the user says, analyzes it and decides the memory topic to trigger next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2308c975-43f1-45f6-9ff5-fb840f1ef4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Remi V6.1 Empathy Layer Loaded\n"
     ]
    }
   ],
   "source": [
    "# --- REMI V6.1: Softer Empathy Layer ---\n",
    "\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Emotion-aware response pools (less clinical, more human)\n",
    "empathy_responses = {\n",
    "    \"bad\": [\n",
    "        \"I'm sorry today feels bad.\",\n",
    "        \"That sounds like a hard day.\",\n",
    "        \"Some days are just heavy.\"\n",
    "    ],\n",
    "    \"sad\": [\n",
    "        \"That sounds really sad.\",\n",
    "        \"I'm here with you.\",\n",
    "        \"I'm glad you told me.\"\n",
    "    ],\n",
    "    \"tired\": [\n",
    "        \"Feeling tired can be exhausting.\",\n",
    "        \"Maybe your body needs some rest.\",\n",
    "        \"We can take things slowly.\"\n",
    "    ],\n",
    "    \"lonely\": [\n",
    "        \"Feeling lonely is really hard.\",\n",
    "        \"You're not alone right now.\",\n",
    "        \"I'm here with you.\"\n",
    "    ],\n",
    "    \"angry\": [\n",
    "        \"That sounds frustrating.\",\n",
    "        \"It's okay to feel upset.\",\n",
    "        \"I hear how strong that feeling is.\"\n",
    "    ],\n",
    "    \"anxious\": [\n",
    "        \"It's okay. You're safe right now.\",\n",
    "        \"Let's slow down together.\",\n",
    "        \"Nothing is urgent.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "grounding_phrases = [\n",
    "    \"You're safe right now.\",\n",
    "    \"Nothing needs to be done.\",\n",
    "    \"We can just sit for a moment.\"\n",
    "]\n",
    "\n",
    "chat_state = {\n",
    "    \"current_topic\": None,\n",
    "    \"asked_questions\": set(),\n",
    "    \"standing_by\": False,\n",
    "    \"comfort_mode\": False\n",
    "}\n",
    "\n",
    "print(\"✅ Remi V6.1 Empathy Layer Loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293650f8-f891-4e0a-8287-6fc875f8f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Remi V10 (Bug Fixed) ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  knock knock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remi: I am listening.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remi: Good evening. I don't know your name yet. What is it?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I am peter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remi: Nice to meet you, I.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is your name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remi: I am listening.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Remi V10 (Bug Fixed) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemi: Goodbye.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1287\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# --- REMI V10: The Honest Bot (Bug Fixes) ---\n",
    "\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "chat_state = {\n",
    "    \"standing_by\": False,\n",
    "    \"waiting_for_name\": False,\n",
    "    \"user_profile\": {\"name\": None}\n",
    "}\n",
    "\n",
    "# --- HELPER: Fixes the \"nothing\" vs \"hi\" bug ---\n",
    "def has_word(word_list, text):\n",
    "    \"\"\"\n",
    "    Checks for WHOLE words only. \n",
    "    'hi' will match 'hi there' but NOT 'nothing' or 'this'.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    for word in word_list:\n",
    "        # \\b means \"word boundary\" in Regex\n",
    "        if re.search(r'\\b' + re.escape(word) + r'\\b', text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_time_greeting():\n",
    "    h = datetime.now().hour\n",
    "    if h < 12: return \"Good morning\"\n",
    "    elif 12 <= h < 18: return \"Good afternoon\"\n",
    "    return \"Good evening\"\n",
    "\n",
    "def process_response(user_input):\n",
    "    text = user_input.lower().strip()\n",
    "    \n",
    "    # --- LAYER 1: WAITING FOR NAME ---\n",
    "    if chat_state[\"waiting_for_name\"]:\n",
    "        # If user refuses\n",
    "        if has_word([\"no\", \"nope\", \"don't\", \"nothing\", \"stop\"], text):\n",
    "            chat_state[\"waiting_for_name\"] = False\n",
    "            return \"Okay. I won't ask for your name. We can just chat.\"\n",
    "        \n",
    "        # Assume input is the name\n",
    "        name = text.split()[0].capitalize() # Take first word\n",
    "        chat_state[\"user_profile\"][\"name\"] = name\n",
    "        chat_state[\"waiting_for_name\"] = False\n",
    "        return f\"Nice to meet you, {name}.\"\n",
    "\n",
    "    # --- LAYER 2: HANDLING \"UPSET\" / \"DON'T ASK\" ---\n",
    "    # This must come BEFORE greetings\n",
    "    if has_word([\"stop\", \"upset\", \"angry\", \"don't\", \"quiet\"], text):\n",
    "        chat_state[\"standing_by\"] = True\n",
    "        return \"I hear you. I will stop asking questions now. I am here if you change your mind.\"\n",
    "\n",
    "    # --- LAYER 3: REALITY CHECK (Weather/Location) ---\n",
    "    if has_word([\"rain\", \"raining\", \"sun\", \"sunny\", \"weather\", \"outside\"], text):\n",
    "        return \"I live inside this computer, so I can't see the sky. But I can check the time! If you look out the window, what do you see?\"\n",
    "\n",
    "    if has_word([\"time\", \"clock\"], text):\n",
    "        now = datetime.now().strftime(\"%I:%M %p\")\n",
    "        return f\"My internal clock says it is {now}.\"\n",
    "\n",
    "    # --- LAYER 4: GREETINGS (Now Fixed!) ---\n",
    "    # Uses has_word so \"nothing\" doesn't trigger \"hi\"\n",
    "    if has_word([\"hello\", \"hi\", \"hey\", \"greetings\"], text):\n",
    "        greeting = get_time_greeting()\n",
    "        name = chat_state[\"user_profile\"][\"name\"]\n",
    "        \n",
    "        if name:\n",
    "            return f\"{greeting}, {name}.\"\n",
    "        else:\n",
    "            chat_state[\"waiting_for_name\"] = True\n",
    "            return f\"{greeting}. I don't know your name yet. What is it?\"\n",
    "\n",
    "    # --- LAYER 5: ACTIVITIES ---\n",
    "    if has_word([\"walk\", \"walking\"], text):\n",
    "        return \"A walk sounds lovely. Do you prefer walking in the city or the park?\"\n",
    "    \n",
    "    if has_word([\"hungry\", \"food\", \"eat\"], text):\n",
    "        return \"You should get something to eat. I can wait here.\"\n",
    "\n",
    "    # --- LAYER 6: FALLBACKS ---\n",
    "    # If user says \"nothing\" or \"no\", just acknowledge it\n",
    "    if has_word([\"nothing\", \"no\", \"nope\"], text):\n",
    "        return \"Okay. We can just sit quietly.\"\n",
    "\n",
    "    return \"I am listening.\"\n",
    "\n",
    "# -----------------------------\n",
    "# INTERACTIVE LOOP\n",
    "# -----------------------------\n",
    "print(\"--- Remi V10 (Bug Fixed) ---\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "        print(\"Remi: Goodbye.\")\n",
    "        break\n",
    "        \n",
    "    response = process_response(user_input)\n",
    "    print(f\"Remi: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9be9d7-0718-46a8-8689-09ef061c0205",
   "metadata": {},
   "source": [
    "The above is important because of context awareness, i.e if the user says I loved my mum, the bot will detect family and ask a family question next. It acts as a safety valve i.e < -0.5 check if the patient gets upset, the bot tries to pivot to a happier subject(music/hobbies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2ac08f-d5e3-4c62-9998-eafc90903ad0",
   "metadata": {},
   "source": [
    "we are now going to stitch all those pieces together into a running loop. This will be the main chat loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed25230-a6c3-4768-aa5b-fac5f1068218",
   "metadata": {},
   "source": [
    "We now want to introduce long term memory to our bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87ab20-6fc0-490a-a7b9-f6006f45e02d",
   "metadata": {},
   "source": [
    "For the next phases, we will do the following: \n",
    "Phase 1 (Done): Rule-Based Logic and it's about understanding how input/output works.\n",
    "\n",
    "Phase 2 (NOW): Data Creation. We need to create a \"textbook\" to teach the bot.\n",
    "\n",
    "Phase 3 (Next): Vectorization. Converting words into numbers (Math).\n",
    "\n",
    "Phase 4 (The Goal): Training a Neural Network. Using TensorFlow/Keras to build a model that learns from your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74294f9e-cd66-47a0-a103-fe83e00e7d64",
   "metadata": {},
   "source": [
    "Step 1: Creating the \"Textbook\" (Training Data)\n",
    "In Machine Learning, we don't write if statements. We create Intents.\n",
    "\n",
    "Intent: What the user means (e.g., greeting, sadness, reminiscence_childhood).\n",
    "\n",
    "Patterns: Examples of how humans say it.\n",
    "\n",
    "Responses: What the bot should say back.\n",
    "\n",
    "We need to create a file called intents.json. This will be the source code for your AI's brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd7b8e0-aab8-4186-a78d-588ede0b4209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training Data Created: 'intents.json'\n",
      "We have stopped writing rules. We are now ready for Machine Learning.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# This is the \"Textbook\" we will feed to the Neural Network\n",
    "data = {\n",
    "  \"intents\": [\n",
    "    {\n",
    "      \"tag\": \"greeting\",\n",
    "      \"patterns\": [\"Hi\", \"How are you\", \"Is anyone there?\", \"Hello\", \"Good day\", \"Whats up\", \"Hey there\"],\n",
    "      \"responses\": [\"Hello!\", \"Good to see you again\", \"Hi there, how can I help?\"],\n",
    "      \"context\": [\"\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"goodbye\",\n",
    "      \"patterns\": [\"cya\", \"See you later\", \"Goodbye\", \"I am Leaving\", \"Have a Good day\", \"bye\", \"quit\"],\n",
    "      \"responses\": [\"Sad to see you go :(\", \"Talk to you later\", \"Goodbye!\"],\n",
    "      \"context\": [\"\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"identity\",\n",
    "      \"patterns\": [\"who are you\", \"what are you\", \"what is your name\", \"are you human\"],\n",
    "      \"responses\": [\"I am Remi, your memory companion.\", \"I am a bot designed to help you share your stories.\"],\n",
    "      \"context\": [\"\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"mood_sad\",\n",
    "      \"patterns\": [\"I feel sad\", \"I am lonely\", \"I feel bad\", \"I am upset\", \"Everything is wrong\", \"I am depressed\"],\n",
    "      \"responses\": [\"I am sorry to hear that. I am here to listen.\", \"It's okay to feel down sometimes. Do you want to talk about it?\"],\n",
    "      \"context\": [\"\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"reminisce_childhood\",\n",
    "      \"patterns\": [\"I was young\", \"When I was a kid\", \"My childhood\", \"I grew up in\", \"school days\"],\n",
    "      \"responses\": [\"Childhood memories are precious. What was your favorite game to play back then?\", \"Tell me about the house you grew up in.\"],\n",
    "      \"context\": [\"\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"confusion\",\n",
    "      \"patterns\": [\"Where am I?\", \"I don't know who I am\", \"I am lost\", \"What is happening\"],\n",
    "      \"responses\": [\"You are safe. You are chatting with Remi.\", \"Take a deep breath. You are safe right here.\"],\n",
    "      \"context\": [\"\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Save this data to a file on your D: drive\n",
    "with open('intents.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "\n",
    "print(\"✅ Training Data Created: 'intents.json'\")\n",
    "print(\"We have stopped writing rules. We are now ready for Machine Learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5377963-3124-4364-8188-f7a9c04c58b6",
   "metadata": {},
   "source": [
    "We will develop a Deep Learning system and we will do it in the following steps:\n",
    "\n",
    "Step 1: The Translation Layer (Text to Numbers)Before we train the brain, we must translate English into a format the brain understands. Neural Networks do not understand text; they only understand Matrices (Numbers).We need to perform Data Preprocessing. This involves three specific steps:\n",
    "\n",
    "i)Tokenization: Breaking sentences into individual words.\n",
    "\n",
    "ii)Stemming: cutting words down to their root form (e.g., \"walking\", \"walked\", \"walks\" $\\rightarrow$ \"walk\"). This stops the bot from thinking \"walked\" is a completely different concept from \"walk\".\n",
    "\n",
    "iii)Bag of Words: Converting those stems into a list of 0s and 1s.\n",
    "\n",
    "Step 2: Preparing the data: loading the intents.json file we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb1b6cef-6953-4c54-bbe4-5a90d552cf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\admin\\anaconda\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: pillow in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\anaconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\admin\\anaconda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\admin\\anaconda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\admin\\anaconda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fcc6b2b-37cb-4240-8836-5666f865649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the specific missing resource\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98065252-ce67-473b-9fe1-0d472a79bcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Processed Successfully!\n",
      "• 33 Documents (Sentences)\n",
      "• 6 Classes (Intents)\n",
      "• 52 Unique Lemmatized Words\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Initialize the Lemmatizer (The tool that cuts words to their root)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Download necessary NLTK data (just in case)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the \"Textbook\" we created\n",
    "with open('intents.json') as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['?', '!', '.', ',']\n",
    "\n",
    "# Loop through every sentence in our intents file\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # 1. Tokenize: Split sentence into words\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        \n",
    "        # 2. Add to documents: Associate patterns with their tag\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        \n",
    "        # 3. Add to classes: Keep track of unique tags (e.g., \"greeting\", \"sadness\")\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# 4. Lemmatize: Lowercase and strip words to root form\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words)) # Remove duplicates\n",
    "\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "print(f\"✅ Data Processed Successfully!\")\n",
    "print(f\"• {len(documents)} Documents (Sentences)\")\n",
    "print(f\"• {len(classes)} Classes (Intents)\")\n",
    "print(f\"• {len(words)} Unique Lemmatized Words\")\n",
    "\n",
    "# Save these for later use (Pickling)\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd92d9c-79b0-4746-b8ea-c1ca50fd8b56",
   "metadata": {},
   "source": [
    "The next step we will do 2 things:\n",
    "\n",
    "1. Create Training Data: Convert our text into \"Bags of Words\" (arrays of 0s and 1s) so the math works.\n",
    "\n",
    "2. Build & Train the Neural Network: We will use Keras (part of TensorFlow) to build a 3-layer neural network and train it on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d979a-73a3-4ec3-9327-3405bd0131a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training Data Created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\ADMIN\\anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Built. Starting Training...\n",
      "Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# --- PART 1: CREATE TRAINING DATA ---\n",
    "\n",
    "training = []\n",
    "# Create an empty array for our output (e.g., [0, 0, 0, 0, 0, 0])\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    # Initialize our bag of words\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    # Lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    \n",
    "    # Create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "    # Output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# Shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "\n",
    "# Create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "\n",
    "print(\"✅ Training Data Created\")\n",
    "\n",
    "# --- PART 2: BUILD THE NEURAL NETWORK ---\n",
    "\n",
    "model = Sequential()\n",
    "# Layer 1: Input Layer (128 neurons, looks at our Bag of Words)\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5)) # Prevents overfitting\n",
    "\n",
    "# Layer 2: Hidden Layer (64 neurons)\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Layer 3: Output Layer (Number of neurons = number of intents)\n",
    "# Softmax gives us a % probability for each tag\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# Compile model. SGD with Nesterov accelerated gradient gives good results for this model\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "print(\"✅ Model Built. Starting Training...\")\n",
    "\n",
    "# --- PART 3: TRAIN THE MODEL ---\n",
    "\n",
    "# We fit the model to the data. \n",
    "# Epochs = How many times it sees the data (200 is standard)\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('remi_model.h5')\n",
    "print(\"✅ Remi is trained and saved as 'remi_model.h5'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea8fa3-5725-442d-98a1-0fd6fa420a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8de4f-cc09-4a1d-b121-2ceea48db3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
